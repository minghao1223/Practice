{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6j5WcA3GbSk"
   },
   "source": [
    "# M2608.001300 기계학습 기초 및 전기정보 응용<br> Assignment 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ei9BMeksGbSn"
   },
   "source": [
    "## Dataset load & Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vlb19coAGbSo"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV9Z9RPMGbSt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.csv', delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "label_mask = np.equal(y, 1)\n",
    "\n",
    "plt.scatter(X[:, 0][label_mask], X[:, 1][label_mask], color='red')\n",
    "plt.scatter(X[:, 0][~label_mask], X[:, 1][~label_mask], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwcWe3pDGbSy"
   },
   "source": [
    "## Problem 1-1. sklearn model로 Logistic Regression 모델 train 시켜보기\n",
    "scikit-learn library의 LogisticRegression 클래스를 이용해 train 시켜 보세요. <br>\n",
    "클래스 인자 및 사용법에 관해서는 scikit-learn 홈페이지의 설명을 참고해 주세요. <br>\n",
    "(참고: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONWQ_Q5yGbS0"
   },
   "outputs": [],
   "source": [
    "def learn_and_return_weights(X, y):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    # YOUR CODE COMES HERE\n",
    "    # w: coefficient of the model to input features,\n",
    "    # b: bias of the model\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HkQB55lkGbS3"
   },
   "outputs": [],
   "source": [
    "def plot_data_and_weights(X, y, w, b):\n",
    "    plt.scatter(X[:, 0][label_mask], X[:, 1][label_mask], color='red')\n",
    "    plt.scatter(X[:, 0][~label_mask], X[:, 1][~label_mask], color='blue')\n",
    "\n",
    "    x_lin = np.arange(20, 70)\n",
    "    y_lin = -(b + w[0] * x_lin) / w[1]\n",
    "\n",
    "    plt.plot(x_lin, y_lin, color='black');\n",
    "    plt.show()\n",
    "\n",
    "w, b = learn_and_return_weights(X, y)\n",
    "plot_data_and_weights(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKYz83ecGbS6"
   },
   "source": [
    "## Problem 1-2. numpy로 Logistic Regression 구현해보기\n",
    "scikit-learn library를 사용하지 않고 Logistic Regression을 구현해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGTRLxvsGbS7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE COMES HERE\n",
    "    return\n",
    "\n",
    "def binary_cross_entropy_loss(y_pred, target):\n",
    "    # YOUR CODE COMES HERE\n",
    "    return\n",
    "    \n",
    "def learn_and_return_weights_numpy(X, Y, lr=.01, iter=100000):\n",
    "    # YOUR CODE COMES HERE\n",
    "    \n",
    "    # w: coefficient of the model to input features,\n",
    "    # b: bias of the model\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryO_ItqRGbS_"
   },
   "outputs": [],
   "source": [
    "w, b = learn_and_return_weights_numpy(X, y)\n",
    "plot_data_and_weights(X, y, w, b)\n",
    "\n",
    "z = np.dot(X, w) + b\n",
    "y_output = sigmoid(z)\n",
    "bce = binary_cross_entropy_loss(y_output,y)\n",
    "print('Binary cross entropy loss:', bce)\n",
    "if (np.isnan(bce) == True) or (bce < 0):\n",
    "    print('You need to make sure your binary cross entropy loss function is correct,\\nor use np.clip to clip the argument of the logarithm from small number (e.g. 1e-10) to 1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTPkme9rGbTC"
   },
   "source": [
    "## Problem 2. sklearn model로 Logistic Regression 모델 train 시켜보기 + regularizer 사용하기\n",
    "scikit-learn library의 Logistic Regression 에 대한 API문서를 읽어보고,<br>\n",
    "L1-regularization을 사용할 때와 L2-regularization을 사용할 때의 weight의 변화를 살펴보세요. <br>\n",
    "(참고: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUR-wC9CGbTC"
   },
   "outputs": [],
   "source": [
    "def learn_and_return_weights_l1_regularized(X, y):    \n",
    "    # YOUR CODE COMES HERE\n",
    "    return w, b\n",
    "\n",
    "def learn_and_return_weights_l2_regularized(X, y):    \n",
    "    # YOUR CODE COMES HERE\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7gpEX1YGbTF"
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    D = 1000\n",
    "    N = 80\n",
    "\n",
    "    X = np.random.random((N, D))\n",
    "    w = np.zeros(D)\n",
    "    w[0] = 1\n",
    "    w[1] = 1\n",
    "    \n",
    "    e = np.random.random(N) - 0.5\n",
    "    \n",
    "    y_score = np.dot(X, w)\n",
    "    y_score_median = np.median(y_score)\n",
    "    print(y_score.max(), y_score.min(), y_score_median)\n",
    "    \n",
    "    # y_score += 0.01 * e\n",
    "    y = y_score >= y_score_median\n",
    "    y = y.astype(np.int32)\n",
    "    \n",
    "    return (X[:N // 2], y[:N // 2]), (X[N // 2:], y[N // 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cG6nG0fGbTI"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = get_dataset()\n",
    "\n",
    "w_l1, b_l1 = learn_and_return_weights_l1_regularized(x_train, y_train)\n",
    "w_l2, b_l2 = learn_and_return_weights_l2_regularized(x_train, y_train)\n",
    "\n",
    "print(w_l1[:5])\n",
    "print(w_l2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "v_DaRCDGHJZ5"
   },
   "source": [
    "## Problem 3-1. Logistic Regression으로 multi-class classification 하기: API 활용하기\n",
    "scikit-learn library의 Logistic Regression API를 활용하면 multi-class classification을 간단하게 수행할 수 있습니다.<br>\n",
    "MNIST dataset에 대해 multi-class classification을 위한 Logistic Regression 모델을 학습시키고, test data에 대한 accuracy를 계산해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJNyQWnuGbTL"
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    from keras.datasets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape((-1, 28 * 28)).astype(np.float32)\n",
    "    x_test = x_test.reshape((-1, 28 * 28)).astype(np.float32)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "(x_train, y_train), (x_test, y_test) = get_dataset()\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Hi81olzGbTT"
   },
   "outputs": [],
   "source": [
    "def learn_mul(X, y):\n",
    "    # YOUR CODE COMES HERE\n",
    "    return lr\n",
    "\n",
    "def inference_mul(x, lr):\n",
    "    # YOUR CODE COMES HERE\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfdTZNUYGbTW"
   },
   "outputs": [],
   "source": [
    "model = learn_mul(x_train, y_train)\n",
    "preds = inference_mul(x_test, model)\n",
    "accuracy = np.sum(preds == y_test) / y_test.shape[0]\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "2OLcsStrGbTK"
   },
   "source": [
    "## Problem 3-2. Logistic Regression으로 multi-class classification 하기: Transformation to Binary\n",
    "\n",
    "Logistic Regression은 기본적으로 binary classifier 입니다. 즉, input *X*를 2개의 class로 밖에 분류하지 못합니다.<br>\n",
    "하지만, 이같은 Logistic Regression 모델을 연달아 사용한다면 data를 여러 class로 분류할 수도 있습니다.<br>\n",
    "(참고: https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary)\n",
    "\n",
    "MNIST dataset을 이용하여 (class 수) 개의 Binary classifier (Logistic Regression)를 'lrs'의 각 원소에 저장한 뒤,<br>\n",
    "학습시킨 모델들을 이용하여 test data에 대한 accuracy를 계산해 보세요.<br>\n",
    "(각 모델의 training iteration은 10회면 충분합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlYDigjwGbTO"
   },
   "outputs": [],
   "source": [
    "def learn_mul2bin(X, y):\n",
    "    lrs = []\n",
    "    ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n/10%10!=1)*(n%10<4)*n%10::4])\n",
    "    for i in range(num_classes):\n",
    "        print('training %s classifier'%(ordinal(i+1)))\n",
    "        # YOUR CODE COMES HERE\n",
    "    return lrs\n",
    "\n",
    "def inference_mul2bin(x, lrs):\n",
    "    # YOUR CODE COMES HERE\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKF3C-4OGbTR"
   },
   "outputs": [],
   "source": [
    "models = learn_mul2bin(x_train, y_train)\n",
    "preds = np.array([inference_mul2bin(x, models) for x in x_test])\n",
    "accuracy = np.sum(preds == y_test) / y_test.shape[0]\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "AS1-Logistic_Regression.ipynb",
   "provenance": [
    {
     "file_id": "1br9I49jKldloi3aOJ6HPGtq3RLU9h5ft",
     "timestamp": 1583728216996
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
